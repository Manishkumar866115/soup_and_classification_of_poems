{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poems.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"../input/poems.csv\"\n",
    "with open(file_path, 'rb') as f:\n",
    "  contents = f.read()\n",
    "\n",
    "contents=contents.decode(encoding='utf-8',errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 192: expected 4 fields, saw 5\\nSkipping line 228: expected 4 fields, saw 5\\nSkipping line 290: expected 4 fields, saw 5\\nSkipping line 294: expected 4 fields, saw 5\\nSkipping line 309: expected 4 fields, saw 5\\nSkipping line 393: expected 4 fields, saw 5\\n'\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "TESTDATA = StringIO(contents)\n",
    "\n",
    "dataset = pd.read_csv(TESTDATA, sep=\",\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "dataset[\"category\"]=le.fit_transform(dataset[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         category\n",
      "count  394.000000\n",
      "mean     1.494924\n",
      "std      1.117169\n",
      "min      0.000000\n",
      "25%      0.250000\n",
      "50%      1.500000\n",
      "75%      2.000000\n",
      "max      3.000000\n"
     ]
    }
   ],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poem_name                                              Lonely Am I \n",
      " poem_content     Lonely are the nights|Lonely are the days|Lone...\n",
      "poet_name                                                 foulk jim\n",
      "category                                                          2\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataset.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating our own corpus with the help of word tokenizer, porter stemmer,stopwords, regular expressions and nltk\n",
    "import re   \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "corpus=[]\n",
    "ps=PorterStemmer()\n",
    "stopwords=set(stopwords.words(\"english\"))\n",
    "for i in range(394):\n",
    "    review=dataset.iloc[i][1]\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset.iloc[i][1] )\n",
    "    review = re.sub('[?&#@$%!_.|,-:;\"]', ' ', dataset.iloc[i][1] )    \n",
    "    review=review.lower()\n",
    "    words=word_tokenize(review)\n",
    "    d=list()\n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            d.append(ps.stem(w))\n",
    "    review=\" \".join(d)\n",
    "    corpus.append(review)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"unhappi far thing affair wander along coast lean ridg saw even star go lone ocean black mane wild boar plow snout mal paso mountain old monster snuffl sweet root fat grub slick beetl sprout acorn best nation europ fallen finland star go lone ocean old black bristl boar tear sod mal paso mountain world 's bad way man bound wors mend better lie mountain four five centuri star go lone ocean said old father wild pig plow fallow mal paso mountain keep clear dupe talk democraci dog talk revolut drunk talk liar believ believ tusk long live freedom damn ideolog said gamey black mane boar tusk turf mal paso mountain submit holt\"]\n"
     ]
    }
   ],
   "source": [
    "#demo of how the corpus looks like\n",
    "print(corpus[5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bag of word using count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=2000)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "y=dataset.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.30,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(X_train, Y_train) \n",
    "\n",
    "y_pred = gnb.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, Y_train)\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23  1  1  4]\n",
      " [ 0 32  1  0]\n",
      " [ 2  0 23  6]\n",
      " [ 3  3  1 19]]\n",
      "[[19  0  3  7]\n",
      " [10 11  5  7]\n",
      " [ 5  3  9 14]\n",
      " [ 7  2  3 14]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm1 = confusion_matrix(Y_test, Y_pred)\n",
    "cm2 = confusion_matrix(Y_test, y_pred)\n",
    "#confusion matrix for xgboost in first with accuracy of nearly 90%\n",
    "print(cm1)\n",
    "#confusion matrix for naive bayes with low accuracy of nearly 60%\n",
    "print(cm2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
